###############################################################
# .env.vllm.example
#
# ParlonaCore – configuration for GPU host using vLLM
#
# Usage:
#   cp .env.vllm.example .env.vllm
#   # edit .env.vllm
#   ./deploy_parlonacore.sh vllm
###############################################################

##############################
# GENERAL
##############################

ENVIRONMENT=production
LOG_LEVEL=info


##############################
# LLM BACKEND SELECTION
##############################

# Use vLLM instead of OpenAI on this host
LLM_BACKEND=vllm


###############################################################
# vLLM SETTINGS
#
# vLLM must expose an OpenAI-compatible API, e.g.:
#   http://<host>:<port>/v1
#
# IMPORTANT:
#   - Keep VLLM_BASE_URL consistent with VLLM_HOST/VLLM_PORT.
#   - If vLLM is remote, set VLLM_HOST to its IP or DNS name.
###############################################################

# If vLLM runs INSIDE the same docker-compose network:
#   VLLM_HOST=vllm
#
# If vLLM runs on the SAME MACHINE but OUTSIDE docker:
#   VLLM_HOST=host.docker.internal   (Linux/Mac; adjust if needed)
#
# If vLLM runs on ANOTHER MACHINE:
#   VLLM_HOST=10.1.2.55  or  llm.mydomain.com
VLLM_HOST=host.docker.internal

# Port where vLLM API listens
VLLM_PORT=8000

# Full base URL used by services to talk to vLLM
# !!! Make sure this matches HOST + PORT above !!!
VLLM_BASE_URL=http://host.docker.internal:8000/v1

# Model name as seen by vLLM (must be available on vLLM host)
# Examples:
#   meta-llama/Meta-Llama-3-8B-Instruct
#   mistralai/Mistral-7B-Instruct-v0.3
#   Qwen/Qwen2.5-7B-Instruct
VLLM_MODEL=meta-llama/Meta-Llama-3-8B-Instruct

# vLLM "API key" – usually not enforced; OpenAI client just wants a token
VLLM_API_KEY=EMPTY



###############################################################
# OPENAI FALLBACK (ignored when LLM_BACKEND=vllm)
###############################################################

OPENAI_API_KEY=
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o-mini



###############################################################
# SPEECH-TO-TEXT (STT) – Faster-Whisper on GPU
###############################################################

# Whisper model for Faster-Whisper
# Options:
#   Systran/faster-whisper-small
#   Systran/faster-whisper-medium
#   Systran/faster-whisper-large-v2
STT_MODEL=Systran/faster-whisper-large-v2

# Use GPU on this host
STT_DEVICE=cuda

# Best default for GPU
STT_COMPUTE_TYPE=float16

STT_BEAM_SIZE=5

# Where to cache Whisper models inside the container
STT_MODEL_CACHE=/models



###############################################################
# DATABASE / REDIS (adjust to your actual setup)
###############################################################

POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=parlonacore
POSTGRES_USER=parlona
POSTGRES_PASSWORD=parlona_password

REDIS_HOST=redis
REDIS_PORT=6379



###############################################################
# OTHER APP-SPECIFIC SETTINGS (fill as needed)
###############################################################

# Example:
# CORE_API_PORT=8080

###############################################################
# END OF FILE
###############################################################