# ParlonaCore version
PARLONACORE_VERSION=1.1.0

# PostgreSQL database configuration
POSTGRES_DB=parlonacore
POSTGRES_USER=parlonacore
POSTGRES_PASSWORD=parlonacore_pass_change_me

# Redis configuration
REDIS_PASSWORD=change_me_to_a_strong_password

# Redis connection
REDIS_URL=redis://:${REDIS_PASSWORD}@localhost:6379/0

# PostgreSQL connection
POSTGRES_DSN=postgresql+psycopg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@localhost:5432/${POSTGRES_DB}

# Run database migrations on startup
RUN_DB_MIGRATIONS=1

# Storage directory for audio files
STORAGE_DIR=./storage

# Model caching configuration
WHISPER_MODEL_DIR=/models/whisper
WHISPER_LOCAL_ONLY=0
HF_HUB_OFFLINE=0

# Job processing configuration
JOB_LIST_LIMIT=100
QUEUE_POLL_TIMEOUT=5

###############################################################
# LLM CONFIGURATION
#
# Choose your LLM backend by setting LLM_BACKEND to one of:
#   openai   - For OpenAI's GPT models
#   vllm     - For self-hosted vLLM models
#   groq     - For Groq's accelerated models
#   ollama   - For locally hosted Ollama models
###############################################################

LLM_BACKEND=openai

# OPENAI CONFIGURATION (used when LLM_BACKEND=openai)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o-mini

# VLLM CONFIGURATION (used when LLM_BACKEND=vllm)
# 
# vLLM must expose an OpenAI-compatible API, e.g.:
#   http://<host>:<port>/v1
#
# IMPORTANT:
#   - Keep VLLM_BASE_URL consistent with VLLM_HOST/VLLM_PORT.
#   - If vLLM is remote, set VLLM_HOST to its IP or DNS name.

# If vLLM runs INSIDE the same docker-compose network:
#   VLLM_HOST=vllm
#
# If vLLM runs on the SAME_MACHINE but OUTSIDE docker:
#   VLLM_HOST=host.docker.internal   (Linux/Mac; adjust if needed)
#
# If vLLM runs on ANOTHER_MACHINE:
#   VLLM_HOST=10.1.2.55  or  llm.mydomain.com
VLLM_HOST=localhost

# Port where vLLM API listens
VLLM_PORT=8000

# Full base URL used by services to talk to vLLM
# !!! Make sure this matches HOST + PORT above !!!
VLLM_BASE_URL=http://localhost:8000/v1

# Model name as seen by vLLM (must be available on vLLM host)
# Examples:
#   meta-llama/Meta-Llama-3-8B-Instruct
#   mistralai/Mistral-7B-Instruct-v0.3
#   Qwen/Qwen2.5-7B-Instruct
VLLM_MODEL=meta-llama/Meta-Llama-3-8B-Instruct

# vLLM "API key" â€“ usually not enforced; OpenAI client just wants a token
VLLM_API_KEY=EMPTY

# GROQ CONFIGURATION (used when LLM_BACKEND=groq)
GROQ_API_KEY=your_groq_api_key_here
GROQ_BASE_URL=https://api.groq.com/openai/v1
GROQ_MODEL=llama3-8b-8192

# OLLAMA CONFIGURATION (used when LLM_BACKEND=ollama)
# 
# Ollama must expose an OpenAI-compatible API, e.g.:
#   http://<host>:<port>/v1
#
# IMPORTANT:
#   - Keep OLLAMA_BASE_URL consistent with OLLAMA_HOST/OLLAMA_PORT.
#   - Make sure the model is pulled in Ollama: `ollama pull llama3`

OLLAMA_HOST=localhost
OLLAMA_PORT=11434
OLLAMA_BASE_URL=http://localhost:11434/v1
OLLAMA_MODEL=llama3


# STT service configuration
STT_DIARIZATION_MODE=stereo_channels
STT_STEREO_SPEAKER_MAPPING=0:manager,1:customer
STT_MODEL_CACHE=/models

# STT CONFIGURATION FOR GPU HOSTS
#
# Uncomment and modify these values if you're running on a GPU host
# STT_ENABLE_GPU=1
# STT_MODEL=Systran/faster-whisper-large-v2
# STT_DEVICE=cuda
# STT_COMPUTE_TYPE=float16
# STT_BEAM_SIZE=5
